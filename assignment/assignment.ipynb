{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a5c461",
      "metadata": {},
      "source": [
        "1) This paper is about how it is often emphasized that cleaning data is a crucial part of data analysis, however, there is a lack of an examination of the various data cleaning techniques and which ones are more effective than others. Through this paper, Wickham seeks to lay out an analysis of a part data cleaning, data tidying, and why data tidying ensures that datasets are easier to work with. By having the structure that variables are colums, observations are rows, and observational units are tables, this makes it easier to also come up with tools that enable easier data tidying.\n",
        "\n",
        "2) The \"tidy data standard\" seeks to create a method of data exploration and analysis that is streamlined and easy to work with, facilitating a focus on the problem at hand rather than spending too much time on tidying the data. Addtionally, it intends to find a way to create data analysis tools in a manner that they can work with each other without having to transform the data in order to input it into another data analysis tool, as seen with other methods.\n",
        "\n",
        "3) The first sentence refers to how each tidy dataset follows certain guidelines so that tidy datasets tend to have similar organizational patterns and are composed of similar characteristics. Conversly, data that is messy are composed of a variety of problems that are specific to that dataset. Families follow a similar structure, as happier familiers tend to have certain of the same characteristics whereas unhappy families may arise due to a multitude of issues.\n",
        "\n",
        "The second sentence refers to how the layout of a dataset enables one to understand broadly that observations will be in the rows and variables will be in the columns. However, the problem arise where although you may be able to locate them, one may not acutually understand what that obersvation or variable means in the context of the dataset. Addtionally, one may not understand what entails an observation or row or what factors contributed to the labeling of a column or the documentation of an observation.\n",
        "\n",
        "4) Wickham defines a value as numbers for quantitative data or strings for qualitative data. A collection of values is what composed a dataset. Within that dataset are variables, which entail values that all measure the same element or feature through units. An observation is made up of all values that have the same units through which they are measured over certain attributes.\n",
        "\n",
        "5) Tidy data is a standardized manner of displaying data where individual variables creates a column, an individual observation creates a row, and a table is created through every one type of observational unit. The arrangement of how rows, columns, and tables are matched with the observations are what determines if a dataset is messy or tidy.\n",
        "\n",
        "6) The first most common problem with messy datasets is that column headers are labeled with values rather than variable names.\n",
        "The second most common problem with messy datasets is that combining and storing multiple variables into one column. \n",
        "The third most common problem with messy datasets is that rows and columns both entails variables\n",
        "The fourth most common problem is that a table will contain various types of observational units.\n",
        "The fifth most common problem is that across multiple table, a single observational unit is kept.\n",
        "The data is table 4 is messy because the column names are composed of the values of income. Addtionally, the question of interest is how income and religion are connected, which should include three variables - religion, which is correctly shown, income, which has values as a header, and frequency, which is not clearly shown with this table. \n",
        "Melting a dataset refers to stacking the data, or changing the columns into rows. This will allow for the values that are headers for columns to become rows.\n",
        "\n",
        "7) Table 11 is messy because it has multiple columns that represent days, however, days should be a value. Addtionally, within the element variable are tmax and tmin. In order to store both the tmax and tmin for each day, the months end up doubling. This is messy because this means these variables are being stored as rows instead of columns. Table 12 is molten because it changed the columns of day values into rows. This also helped reduce columns by having the year, month, and day together as one under the date column. It is ultimatly made tidy by turning the tmax and tmin rows into columns. This makes the dataset tidy because now each column is a variable and the rows are actual values.\n",
        "\n",
        "8) The \"chicken and egg\" problem with tidy data refers to how creating tidy data by utilizing specific tidy tools leads to those tools being soley connected to tidy data. This would make it harder to modify individual data structures or certain data tools without having to adhere to what has already been established through the reciporal connection of the construction of tidy data allows for more tools to be installed and it is these tools that give rise to tidy data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "db7357cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
            " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
            " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
            " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
            " '225' '600' '109' '1,990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
            " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
            " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
            " '249' '192' '142' '169' '1,000' '131' '138' '113' '122' '329' '101' '475'\n",
            " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
            " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
            " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
            " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
            " '229' '425' '67' '87' '1,200' '158' '650' '234' '310' '695' '400' '166'\n",
            " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
            " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
            " '279' '330' '5,000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
            " '30' '3,000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
            " '171' '157' '545' '1,500' '83' '96' '1,800' '81' '188' '380' '255' '505'\n",
            " '54' '33' '174' '93' '740' '640' '1,300' '440' '599' '357' '1,239' '495'\n",
            " '127' '5,999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
            " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
            " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
            " '201' '2,750' '4,750' '432' '675' '167' '390' '298' '339' '194' '302'\n",
            " '211' '595' '191' '53' '361' '480' '8,000' '4,500' '459' '997' '345'\n",
            " '216' '218' '111' '735' '276' '91' '490' '850' '398' '36' '775' '267'\n",
            " '625' '336' '2,500' '176' '725' '3,750' '469' '106' '460' '287' '575'\n",
            " '227' '263' '25' '228' '208' '177' '880' '148' '116' '685' '470' '217'\n",
            " '164' '61' '645' '699' '405' '252' '319' '268' '419' '343' '525' '311'\n",
            " '840' '154' '294' '950' '409' '184' '257' '204' '241' '2,000' '412' '121'\n",
            " '288' '196' '900' '647' '524' '1,750' '309' '510' '1,495' '1,700' '799'\n",
            " '383' '372' '492' '327' '1,999' '656' '224' '173' '875' '1,170' '795'\n",
            " '690' '146' '465' '1,100' '151' '274' '429' '825' '282' '256' '1,111'\n",
            " '620' '271' '161' '51' '855' '579' '1,174' '430' '20' '899' '649' '485'\n",
            " '181' '455' '4,000' '243' '342' '590' '560' '374' '437' '232' '359' '985'\n",
            " '31' '244' '254' '723' '237' '428' '370' '34' '1,400' '580' '2,520' '221'\n",
            " '749' '1,600' '2,695' '306' '202' '680' '570' '520' '223' '2,295' '213'\n",
            " '1,065' '346' '24' '286' '296' '266' '26' '995' '1,368' '393' '182' '635'\n",
            " '258' '780' '589' '347' '1,250' '1,350' '446' '3,200' '1,050' '1,650'\n",
            " '1,550' '975' '323' '6,500' '2,499' '1,850' '2,250' '715' '461' '540'\n",
            " '356' '439' '384' '569' '1,900' '22' '785' '626' '830' '318' '444' '321'\n",
            " '401' '1,499' '888' '369' '770' '386' '366' '344' '630' '313' '597' '262'\n",
            " '509' '10,000' '278' '312' '789' '1,195' '422' '21' '765' '3,500' '945'\n",
            " '326' '3,100' '2,486' '3,390' '1,356' '2,599' '472' '454' '328' '396'\n",
            " '291']\n",
            "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
            " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
            " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
            " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
            " '225' '600' '109' '1990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
            " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
            " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
            " '249' '192' '142' '169' '1000' '131' '138' '113' '122' '329' '101' '475'\n",
            " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
            " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
            " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
            " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
            " '229' '425' '67' '87' '1200' '158' '650' '234' '310' '695' '400' '166'\n",
            " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
            " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
            " '279' '330' '5000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
            " '30' '3000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
            " '171' '157' '545' '1500' '83' '96' '1800' '81' '188' '380' '255' '505'\n",
            " '54' '33' '174' '93' '740' '640' '1300' '440' '599' '357' '1239' '495'\n",
            " '127' '5999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
            " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
            " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
            " '201' '2750' '4750' '432' '675' '167' '390' '298' '339' '194' '302' '211'\n",
            " '595' '191' '53' '361' '480' '8000' '4500' '459' '997' '345' '216' '218'\n",
            " '111' '735' '276' '91' '490' '850' '398' '36' '775' '267' '625' '336'\n",
            " '2500' '176' '725' '3750' '469' '106' '460' '287' '575' '227' '263' '25'\n",
            " '228' '208' '177' '880' '148' '116' '685' '470' '217' '164' '61' '645'\n",
            " '699' '405' '252' '319' '268' '419' '343' '525' '311' '840' '154' '294'\n",
            " '950' '409' '184' '257' '204' '241' '2000' '412' '121' '288' '196' '900'\n",
            " '647' '524' '1750' '309' '510' '1495' '1700' '799' '383' '372' '492'\n",
            " '327' '1999' '656' '224' '173' '875' '1170' '795' '690' '146' '465'\n",
            " '1100' '151' '274' '429' '825' '282' '256' '1111' '620' '271' '161' '51'\n",
            " '855' '579' '1174' '430' '20' '899' '649' '485' '181' '455' '4000' '243'\n",
            " '342' '590' '560' '374' '437' '232' '359' '985' '31' '244' '254' '723'\n",
            " '237' '428' '370' '34' '1400' '580' '2520' '221' '749' '1600' '2695'\n",
            " '306' '202' '680' '570' '520' '223' '2295' '213' '1065' '346' '24' '286'\n",
            " '296' '266' '26' '995' '1368' '393' '182' '635' '258' '780' '589' '347'\n",
            " '1250' '1350' '446' '3200' '1050' '1650' '1550' '975' '323' '6500' '2499'\n",
            " '1850' '2250' '715' '461' '540' '356' '439' '384' '569' '1900' '22' '785'\n",
            " '626' '830' '318' '444' '321' '401' '1499' '888' '369' '770' '386' '366'\n",
            " '344' '630' '313' '597' '262' '509' '10000' '278' '312' '789' '1195'\n",
            " '422' '21' '765' '3500' '945' '326' '3100' '2486' '3390' '1356' '2599'\n",
            " '472' '454' '328' '396' '291']\n",
            "[  145    37    28   199   549   149   250    90   270   290   170    59\n",
            "    49    68   285    75   100   150   700   125   175    40    89    95\n",
            "    99   499   120    79   110   180   143   230   350   135    85    60\n",
            "    70    55    44   200   165   115    74    84   129    50   185    80\n",
            "   190   140    45    65   225   600   109  1990    73   240    72   105\n",
            "   155   160    42   132   117   295   280   159   107    69   239   220\n",
            "   399   130   375   585   275   139   260    35   133   300   289   179\n",
            "    98   195    29    27    39   249   192   142   169  1000   131   138\n",
            "   113   122   329   101   475   238   272   308   126   235   315   248\n",
            "   128    56   207   450   215   210   385   445   136   247   118    77\n",
            "    76    92   198   205   299   222   245   104   153   349   114   320\n",
            "   292   226   420   500   325   307    78   265   108   123   189    32\n",
            "    58    86   219   800   335    63   229   425    67    87  1200   158\n",
            "   650   234   310   695   400   166   119    62   168   340   479    43\n",
            "   395   144    52    47   529   187   209   233    82   269   163   172\n",
            "   305   156   550   435   137   124    48   279   330  5000   134   378\n",
            "    97   277    64   193   147   186   264    30  3000   112    94   379\n",
            "    57   415   236   410   214    88    66    71   171   157   545  1500\n",
            "    83    96  1800    81   188   380   255   505    54    33   174    93\n",
            "   740   640  1300   440   599   357  1239   495   127  5999   178   348\n",
            "   152   242   183   253   750   259   365   273   197   397   103   389\n",
            "   355   559    38   203   999   141   162   333   698    46   360   895\n",
            "    10    41   206   281   449   388   212   102   201  2750  4750   432\n",
            "   675   167   390   298   339   194   302   211   595   191    53   361\n",
            "   480  8000  4500   459   997   345   216   218   111   735   276    91\n",
            "   490   850   398    36   775   267   625   336  2500   176   725  3750\n",
            "   469   106   460   287   575   227   263    25   228   208   177   880\n",
            "   148   116   685   470   217   164    61   645   699   405   252   319\n",
            "   268   419   343   525   311   840   154   294   950   409   184   257\n",
            "   204   241  2000   412   121   288   196   900   647   524  1750   309\n",
            "   510  1495  1700   799   383   372   492   327  1999   656   224   173\n",
            "   875  1170   795   690   146   465  1100   151   274   429   825   282\n",
            "   256  1111   620   271   161    51   855   579  1174   430    20   899\n",
            "   649   485   181   455  4000   243   342   590   560   374   437   232\n",
            "   359   985    31   244   254   723   237   428   370    34  1400   580\n",
            "  2520   221   749  1600  2695   306   202   680   570   520   223  2295\n",
            "   213  1065   346    24   286   296   266    26   995  1368   393   182\n",
            "   635   258   780   589   347  1250  1350   446  3200  1050  1650  1550\n",
            "   975   323  6500  2499  1850  2250   715   461   540   356   439   384\n",
            "   569  1900    22   785   626   830   318   444   321   401  1499   888\n",
            "   369   770   386   366   344   630   313   597   262   509 10000   278\n",
            "   312   789  1195   422    21   765  3500   945   326  3100  2486  3390\n",
            "  1356  2599   472   454   328   396   291]\n"
          ]
        }
      ],
      "source": [
        "# Number 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "airbnb = pd.read_csv(\"/Users/borayadiul/Desktop/DS 3001/wrangling/assignment/data/airbnb_hw.csv\", low_memory=False)\n",
        "price1 = airbnb['Price']\n",
        "print(price1. unique())\n",
        "price2 = price1.str.replace(\",\", \"\")\n",
        "print(price2.unique())\n",
        "price3 = pd.to_numeric(price2, errors='coerce')\n",
        "print(price3.unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a629c980",
      "metadata": {},
      "source": [
        "For this problem, I first imported the data using pd.read_csv, since the original file was a .csv file.\n",
        "I then created a vector, price1, that only included the price vector in order to focus my cleaning on that vector.\n",
        "After, I printed out the unique values within price1 in order to see this data and what needed to be cleaned.\n",
        "I noticed that the values had single quotation marks, indicating that they were strings and addtionally, values over 999 had a comma for the thousands.\n",
        "I decided to remove the commas as well as change the data type from string to numeric to make it easier to use this data for further analysis, and to make computing difference statistics cleaner. I assigned these changes to a new vector to ensure that I could go back to unedited data in case of a mistake.\n",
        "I then used .unique() for the cleaned data to ensure that the commas and single quotation marks were gone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "df21d74a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type\n",
            "Unprovoked             4716\n",
            "Provoked                593\n",
            "Invalid                 552\n",
            "Sea Disaster            239\n",
            "Watercraft              142\n",
            "Boat                    109\n",
            "Boating                  92\n",
            "Questionable             10\n",
            "Unconfirmed               1\n",
            "Unverified                1\n",
            "Under investigation       1\n",
            "Boatomg                   1\n",
            "Name: count, dtype: int64\n",
            "5\n",
            "0       Unprovoked\n",
            "1         Provoked\n",
            "2       Unprovoked\n",
            "3       Unprovoked\n",
            "4       Unprovoked\n",
            "           ...    \n",
            "6457    Unprovoked\n",
            "6458    Unprovoked\n",
            "6459    Unprovoked\n",
            "6460    Unprovoked\n",
            "6461    Unprovoked\n",
            "Name: Type, Length: 6457, dtype: object\n",
            "0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Type\n",
              "Unprovoked           4716\n",
              "Provoked              593\n",
              "Needs_Examination     565\n",
              "Vessel_related        344\n",
              "Sea Disaster          239\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number 2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "sharks = pd.read_csv(\"/Users/borayadiul/Desktop/DS 3001/wrangling/assignment/data/sharks.csv\", low_memory=False)\n",
        "type1 = sharks[\"Type\"]\n",
        "print(type1.value_counts())\n",
        "print(sum(type1.isnull()))\n",
        "\n",
        "type1 = type1.replace(['Watercraft', 'Boat','Boating','Boatomg'], \"Vessel_related\")\n",
        "type1 = type1.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],\"Needs_Examination\")\n",
        "clean_type1 = type1.dropna()\n",
        "print(clean_type1)\n",
        "print(sum(clean_type1.isnull()))\n",
        "clean_type1.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa9896b",
      "metadata": {},
      "source": [
        "For this question, I also imported the data using pd.read_csv. I then created a new vector that only included the type column for the sharks data in order to make cleaning easier and direct. I used value.counts() to see just how many times a unique value appears within the type vector. I did this to see if there were any categories that could be grouped together on the basis of similar characteristics of the incident, which would help with the overall analysis. Following that, I looked at if there were any null values that needed to be cleaned. After see that there were no null values, I then updated the type1 vector by grouping together Watercraft, Boat, Boating, and Boatomg as Vessel_related and replacing those from the original data. I did this because it seemed like all of these ones had some type of water vehicle involed with the incident. Next, I updated type1 again by grouping together Invalid, Questionable, Unconfirmed, Unverified, and Under Investigation and encompassing adn replacing those with Needs_examination. I did this because all of those cases seemed to be not fully understood or reliably documented. After, I dropped any possible na values from the newly grouped type1 data, put that in a new vector, and printed that new vector. I then rechecked if there were any null values as well as the unique value count to ensure that they changes I made were correctly done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "6817d33e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        9\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "22981    1\n",
            "22982    1\n",
            "22983    1\n",
            "22984    1\n",
            "22985    1\n",
            "Name: WhetherDefendantWasReleasedPretrial, Length: 22986, dtype: int64\n",
            "WhetherDefendantWasReleasedPretrial\n",
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: count, dtype: int64\n",
            "0\n",
            "WhetherDefendantWasReleasedPretrial\n",
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Number 3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "justice1 = pd.read_parquet(\"/Users/borayadiul/Desktop/DS 3001/wrangling/data/justice_data.parquet\", engine='pyarrow')\n",
        "released1 = justice1['WhetherDefendantWasReleasedPretrial']\n",
        "print(released1)\n",
        "print(released1.value_counts())\n",
        "print(sum(released1.isnull()))\n",
        "released2 = released1.replace(9, np.nan)\n",
        "print(released2.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30177f11",
      "metadata": {},
      "source": [
        "For this question, since the data was in the format of parquet, I used pd.read_parquet in order to read the data in. I then created a new vector that only included the 'WhetherDefendantWasReleasedPretrial'column from the original dataset. Following that, I printed the new vector as well as print the unique value counts to get a sense of what the vector looks like. The value counts also helped me see which specific values were used to organize the data because I was then able to go to the codebook and see what each of the number meant for this dataset. Next, I printed if there are any null values within the released1 vector to see if those needed to be cleaned. After looking at the codebook, they coded 9 to mean 'Unclear', indicating that this data was could not reliable say whether someone had been released or not definitively. Further analysis of this data would probaly be focused on the clearly documented cases of whether someone was released (1) or not (0), therefore I decided to replace the obeservations that were coded as 9 with nan to inidcate that these values were 'missing' or undefined within the vector and store this cleaned data within a new vector. I did this to ensure that I still had copies of unedited data in cases there was a mistake. Finally, I printed out the unique value counts for the newly cleaned vector to make sure that the data was cleaned properly, the 9 values were taken out, and the original count for 1 and 0 remained unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImposedSentenceAllChargeInContactEvent\n",
            "                    9053\n",
            "0                   4953\n",
            "12                  1404\n",
            ".985626283367556    1051\n",
            "6                    809\n",
            "                    ... \n",
            "49.9712525667351       1\n",
            "57.0349075975359       1\n",
            "79.9260780287474       1\n",
            "42.1642710472279       1\n",
            "1.6570841889117        1\n",
            "Name: count, Length: 484, dtype: int64\n",
            "0\n",
            "Total Missings: \n",
            " 9053 \n",
            "\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914     0    0\n",
            "True                                                 0     0    0  8779  274\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274\n",
            "Total Missings: \n",
            " 274\n"
          ]
        }
      ],
      "source": [
        "# Number 4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "justice1 = pd.read_parquet(\"/Users/borayadiul/Desktop/DS 3001/wrangling/data/justice_data.parquet\", engine='pyarrow')\n",
        "imposed1 = justice1['ImposedSentenceAllChargeInContactEvent']\n",
        "print(imposed1.value_counts())\n",
        "print(sum(imposed1.isnull()))\n",
        "sen_type1 = justice1[\"SentenceTypeAllChargesAtConvictionInContactEvent\"]\n",
        "sen_type1.value_counts()\n",
        "\n",
        "imposed1 = pd.to_numeric(imposed1, errors='coerce')\n",
        "imposed1_nan = imposed1.isnull()\n",
        "print('Total Missings: \\n', sum(imposed1_nan),'\\n') \n",
        "print(pd.crosstab(imposed1_nan,sen_type1))\n",
        "\n",
        "imposed1 = imposed1.mask(sen_type1 ==4, 0)\n",
        "imposed1 = imposed1.mask(sen_type1 ==9, np.nan)\n",
        "\n",
        "imposed1_nan = imposed1.isnull()\n",
        "print(pd.crosstab(imposed1_nan, sen_type1))\n",
        "\n",
        "print('Total Missings: \\n', np.sum(imposed1_nan)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a044d6",
      "metadata": {},
      "source": [
        "For this question, I again imported the data using pd.read_parquet since the data was in a parquet format. Next, I created a new vector that only contained the 'ImposedSentenceAllChargeInContactEvent' column from the original dataset. I printed the unique value counts to see how the data looked like and I also printed the amount of null values that were present within the vector. In order to have more context behind what made up the total imposed sentence, I decided to also create a new vector that contained the 'SentenceTypeAllChargesAtConvictionInContactEvent' variable from the original dataset. The type of sentence one receives impacts the amount of time they may need to serve their sentences, as someone who was assigned a charge sentence that was later dropped will appear as missing in the imposed vector. I also did a unique value count of the sentence type vector to see the distribution, as well as to see what values to look up in the codebook to see what they indicate. Based on what was shown in the value counts for imposed1, I converted this vector into numeric values to get rid of any spaces and to make further analysis easier. After seeing that there were a large number of null, missing values, I placed those values into a dummy variable and printed out the total number of missing values to ensure it matched the amount of missing values I originally printed earlier. Then, I created a table looking at the missing values in the imposed vector alongside the sentence type. I did this to see which category of sentencing these missing values were falling into and to be able to see what value in the codebook that corresponded too. \n",
        "With true indicating the count of missing data and false indicating nonmissing data, the table revealed that 4 and 9 from the sentencing type vector contained the missing data. 4 indicates that the charges were pending, dimissed, or deffered and 9 indicates that the observation was no longer present. I decided to replace 4 with 0, to represent how those charges were not acutally missing data, and 9 with nan, to represent how those obersvations were missing. \n",
        "I then created another dummy variable for missing data to account for the changes in what should be counted as missing compared to what data is still present and created another table with this dummy variable and the sentence type to ensure that it properly documented that only 9 should have true values, indicating missing data. Finally, I printed out the total number of missing data in the dummy variable to make sure it matches what was in the table and different from the orignal number of 9053."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
